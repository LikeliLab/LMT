{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381baf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(239)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aadfead",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This educational notebook delves into the mathematics of the Transformer architecture, \n",
    "providing an implementation of its main components in PyTorch. \n",
    "This architecture is the core technology behind large language models (LLMs), \n",
    "such as OpenAI's GPT models.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Attention Weights](#Attention-Weights)\n",
    "2. [Self-Attention Mechanism Without Trainable Parameters](#Self-Attention-Mechanism-Without-Trainable-Parameters)\n",
    "3. [Adding Trainable Weights to Self-Attention](#Adding-Trainable-Weights-to-Self-Attention)\n",
    "4. [Multi-head Attention](#Multi-head-Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f6a20",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d57ae4",
   "metadata": {},
   "source": [
    "  \n",
    "The input data to a transfomer is a set of vectors $x_1, \\cdots, x_N$ each with a\n",
    "dimensionality of $D$. This, of course, can we written as a $D \\times N$ matrix $\\mathbf{X}$. \n",
    "In (Bishop & Bishop, 2023) the $x_1 \\cdots x_n$ vectors are referred to as \"tokens.\" \n",
    "I will refer to these as token embedding vectors or token embeddings for short. \n",
    "In language modeling, through a process known as \"tokenization\" a sentence or \n",
    "sequence of words in split into a sequence of natural number which I will \n",
    "refer to as \"tokens.\" Each token is then converted to a $D$ dimenionsal \n",
    "vector (i.e. token embedding) to create the input for the transformer.\n",
    "\n",
    "Now suppose we have a sentence (e.g. \"I swam across the river to get to the other bank\") and this\n",
    "sentence has gone through the tokenization process and has been converted into a sequence of \n",
    "token embedding vectors, $x_1, \\cdots, x_N$, each with dimensionality of $D$. Now goal is to map this\n",
    "sequence of vectors $x_1, \\cdots, x_N$ to a new sequence of vectors $y_1, \\cdots, y_N$\n",
    "in a new space that captures important \"semantic\" information within the full sentence. \n",
    "For example, when reading the sentence \"I swam across the river to get to the other bank\" \n",
    "the words \"swam\" and \"river\" give us information about the meaning of \"bank\" in that sentence.\n",
    "As in (Raschka, 2024) we will refer to the $y_n$ as a context vector to highlight\n",
    "that each $y_n$ depends on the other token embeddings in the token embedding sequence or \"context.\"\n",
    "\n",
    "To this end, we want the context vector $y_n$ to depend not only on $x_n$ but also on\n",
    "the full token embedding sequence $x_1, \\cdots, x_N$. For those that more familar with\n",
    "tokenizers will ignore masking for now. Returning to our example sentence, \n",
    "this would mean the context vector $y_n$ would depend not only on the \n",
    "embedding vector $x_n$ corresponding to the word \"bank\" but on \n",
    "every embedding vector in the sequence, each corresponding to a word (i.e. token) \n",
    "in the sentence.\n",
    "\n",
    "A possible approach to constructing the context vectors $y_n$ is to define each $y_n$ \n",
    "to be a linear combination of the token embeddings $x_1, \\cdots n_N$. That is, $y_n = \\sum_{m=1}^N a_{nm}x_m$.\n",
    "The question then become how do we define the weights $a_{nm}$. To start we constrain the\n",
    "weights to be non-negative and sum to one. Mathematically, $a_{nm} \\geq 0$ for $n, m=1, \\cdots N$ and $\\sum_{m=1}^N a_{nm}=1$ for $n=1, \\cdots N$.\n",
    "\n",
    "In summary, we have\n",
    "$$\n",
    "y_n = \\sum_{m=1}^N a_{nm}x_m\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "a_{nm} \\geq 0 \\quad n, m=1, \\cdots N \\quad \\text{and}\\\\\n",
    "\\sum_{m=1}^N a_{nm}=1 \\quad n=1, \\cdots N.\n",
    "$$\n",
    "The $a_{nm}$ parameters are called **attention weights**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fab40",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism Without Trainable Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df68703",
   "metadata": {},
   "source": [
    "Self-Attention is the name given to a process for calculating the attention weights. \n",
    "Many of the terms used here are from the field of information retrieval and so we\n",
    "will began with some definitions. If you are familiar with Python then recall \n",
    "a dictionary object. Dictionaries in Python have **keys** and **values** (e.g. {'key': 'value'}).\n",
    "Now given a dictionary a user provides a query. The query is what they want to find in the dictionary. \n",
    "The query is used to look up (or check) for the key in the dictionary and when a match is made\n",
    "the value is returned. This looks like this in Python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b5ecf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: self, Value: attention\n"
     ]
    }
   ],
   "source": [
    "example_dictionary = collections.defaultdict(str)\n",
    "example_dictionary['self'] = 'attention'  # {key1: value1}\n",
    "example_dictionary['attention'] = 'weights'  # {key2: value2}\n",
    "example_dictionary['python'] = 'dictionary'  # {key3: value3}\n",
    "example_dictionary['trainable'] = 'weights'  # {key3: value3}\n",
    "\n",
    "query = 'self'\n",
    "value = example_dictionary.get(query, 'not found')\n",
    "print(f'Query: {query}, Value: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1b55e",
   "metadata": {},
   "source": [
    "The Python dictionary example is just one simple example of the idea behind key, value, and query triples.\n",
    "You can imagine extending this so that the query and key do not have to match exactly as they did in our dictionary example.\n",
    "We could take our query and search for the most \"similar\" (we have to define what similar means). This is idea will help us understand\n",
    "the self-attention mechanism.\n",
    "\n",
    "In the self-attention mechanism the sequences of token embedding vectors, ${x_1, \\cdots x_N}$, can be thought of as values and these values will be used to create the context vectors. Furthermore, we will also use sequences of token embedding vectors,  ${x_1, \\cdots x_N}$, directly as the keys for each corresponding value. So using the dictionary example before you could imagine our dictionary looks like $\\{x_1:x_1, x_2:x_2, \\cdots, x_N:x_N\\}$. This example is to simply an analogy to help our understanding we will of course not acutally be using these dictionaries in the attention mechanism. Now consider a single token embedding vector, $x_m$, from our sequence of token embedding vectors, ${x_1, \\cdots x_N}$. The token embedding vector $x_m$ will be our query. For each $x_m$ will then measure the \"degree of match\" (or similarity) between the $x_m$ and all the keys (our sequence of token embedding vectors ${x_1, \\cdots x_N}$). This measure of \"degree of match\" will then be used as the weights in a linear combination of the values to produce the context embedding vector $y_m$. We will repeat this produce treating each embedding vector as a query and using it to find the weights to use in a linear combination of the values to produce the corresponding context embedding vector. Of course using matrix multiple we can calculate all the necessary weights at once.\n",
    "\n",
    "A common method to determine the degree of match (or similarity) between a query vector and a key vector is to take their [dot product](https://en.wikipedia.org/wiki/Dot_product). Let $a^*_{nm}$ be the unconstrained attention weights. We will refer to these as attention scores as in (Raschka, 2024). Then for a given query $x_n$ and key $x_m$ the attention score is defined to be $a^*_{nm} = x^T_n x_m$. To constrain the values to be non-negative and sum to one we will apply the [softmax function](https://en.wikipedia.org/wiki/Softmax_function). This results in the attention weights:\n",
    "$$\n",
    "a_{nm} = \\frac{\\exp(a^*_{nm})}{\\sum_{k=1}^N \\exp(a^*_{nk})} = \\frac{\\exp(x^T_n x_m)}{\\sum_{k=1}^N \\exp(x^T_n x_k)}.\n",
    "$$\n",
    "\n",
    "Each attention weight $a_{nm}$ is the contribution of the key $x_m$ to the output context vector $y_n$. Mathematically, \n",
    "$$\n",
    "y_n = \\sum_{m=1}^N a_{nm}x_m.\n",
    "$$\n",
    "\n",
    "Let $\\mathbf{X}$ be a matrix of token embedding vectors where each **row** corresponds to a token embedding, $x_n$ and let $\\mathbf{A^*}$ be the matrix of unconstrained attention scores (before the softmax function has been applied). Then we have $\\mathbf{A^*} = \\mathbf{X}\\mathbf{X^T}$ where each row of $A^*$ is a vector of scores that when constrained using the softmax function will determine how much each input token embedding vector contributes to the linear combination that produces the context vector $y_n$. Applying the softmax function across the rows we get $A = \\text{Softmax}[\\mathbf{XX^T}] = \\text{Softmax}[\\mathbf{A^*}]$ where the softmax function is apply across the rows independently. Finally we can apply this attention weights to the values (another set of the input token embeddings token embedding) using matrix multiplcation. Let $\\mathbf{Y}$ be the matrix of output context vectors where each **row** is a context vector in the new (hopefully) more contextually rich space. Then \n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}[\\mathbf{XX^t}]\\mathbf{X}  = \\text{Softmax}[\\mathbf{A^*}]\\mathbf{X} = \\mathbf{AX}.\n",
    "$$\n",
    "\n",
    "Now lets see how to program this in [PyTorch](https://pytorch.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b96f65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tensor([[0.5341, 0.3316, 0.5995],\n",
      "        [0.9891, 0.8921, 0.4602],\n",
      "        [0.1234, 0.5678, 0.9101],\n",
      "        [0.4567, 0.7890, 0.1234],\n",
      "        [0.2345, 0.6789, 0.3456]])\n",
      "\n",
      "A_star = tensor([[0.7546, 1.1000, 0.7998, 0.5795, 0.5576],\n",
      "        [1.1000, 1.9859, 1.0474, 1.2124, 0.9966],\n",
      "        [0.7998, 1.0474, 1.1659, 0.6167, 0.7289],\n",
      "        [0.5795, 1.2124, 0.6167, 0.8463, 0.6854],\n",
      "        [0.5576, 0.9966, 0.7289, 0.6854, 0.6353]])\n",
      "\n",
      "A = tensor([[0.1953, 0.2759, 0.2044, 0.1640, 0.1604],\n",
      "        [0.1564, 0.3793, 0.1484, 0.1750, 0.1410],\n",
      "        [0.1822, 0.2334, 0.2628, 0.1517, 0.1698],\n",
      "        [0.1578, 0.2971, 0.1637, 0.2060, 0.1754],\n",
      "        [0.1679, 0.2605, 0.1993, 0.1908, 0.1815]])\n",
      "\n",
      "Y = tensor([[0.5150, 0.6652, 0.5058],\n",
      "        [0.5899, 0.7082, 0.4736],\n",
      "        [0.4698, 0.6529, 0.5333],\n",
      "        [0.5335, 0.6919, 0.4664],\n",
      "        [0.5016, 0.6750, 0.4882]])\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have a sentence of 5 words or \"tokens\"\n",
    "# (e.g. \"This is an example sentence.\")\n",
    "# Then we can represent each token as a vector of dimension d (e.g. d=4).\n",
    "\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [0.5341, 0.3316, 0.5995],  # This        x1\n",
    "        [0.9891, 0.8921, 0.4602],  # is          x2\n",
    "        [0.1234, 0.5678, 0.9101],  # an          x3\n",
    "        [0.4567, 0.7890, 0.1234],  # example     x4\n",
    "        [0.2345, 0.6789, 0.3456],  # sentence    x5\n",
    "    ]\n",
    ")\n",
    "print(f'X = {X}\\n')\n",
    "\n",
    "A_star = X @ X.T  # Unconstrained attention scores\n",
    "print(f'A_star = {A_star}\\n')\n",
    "\n",
    "# Output:\n",
    "# A_star = tensor([[0.7546, 1.1000, 0.7998, 0.5795, 0.5576],\n",
    "#                  [1.1000, 1.9859, 1.0474, 1.2124, 0.9966],\n",
    "#                  [0.7998, 1.0474, 1.1659, 0.6167, 0.7289],\n",
    "#                  [0.5795, 1.2124, 0.6167, 0.8463, 0.6854],\n",
    "#                  [0.5576, 0.9966, 0.7289, 0.6854, 0.6353]])\n",
    "\n",
    "# Apply softmax across rows to constrain attention scores\n",
    "A = torch.softmax(A_star, dim=1)\n",
    "print(f'A = {A}\\n')\n",
    "\n",
    "# Output:\n",
    "# A = tensor([[0.1953, 0.2759, 0.2044, 0.1640, 0.1604],\n",
    "#         [0.1564, 0.3793, 0.1484, 0.1750, 0.1410],\n",
    "#         [0.1822, 0.2334, 0.2628, 0.1517, 0.1698],\n",
    "#         [0.1578, 0.2971, 0.1637, 0.2060, 0.1754],\n",
    "#         [0.1679, 0.2605, 0.1993, 0.1908, 0.1815]])\n",
    "#\n",
    "# Looking at the first row of A, we see that to compute y1, the context vector\n",
    "# for the first token embedding x1 (\"This\"), we will weight x1 (itself) by\n",
    "# 0.1953, x2 (\"is\") by 0.2759, x3 (\"an\") by 0.2044, x4 (\"example\") by 0.1640,\n",
    "# and x5 (\"sentence\") by 0.1604. This means that the context.\n",
    "\n",
    "# Compute the context vectors using matrix multiplication\n",
    "Y = A @ X\n",
    "print(f'Y = {Y}')  # notice that Y is the same shape as X\n",
    "\n",
    "# Y = tensor([[0.5150, 0.6652, 0.5058], # This      y1\n",
    "#         [0.5899, 0.7082, 0.4736],     # is        y2\n",
    "#         [0.4698, 0.6529, 0.5333],     # an        y3\n",
    "#         [0.5335, 0.6919, 0.4664],     # example   y4\n",
    "#         [0.5016, 0.6750, 0.4882]])    # sentence  y5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8ffe1",
   "metadata": {},
   "source": [
    "## Adding Trainable Parameters to Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6418a446",
   "metadata": {},
   "source": [
    "At this point the transformation from an token embedding vector, $x_1$ to a context\n",
    "vector $y_1$ is fixed with no ability to \"good\" representations from the data.\n",
    "To this end we will add weights that through training will allow us to learn\n",
    "good context context vectors, $y_1$. In this context \"good\" means the context vectors are useful\n",
    "for the task we want to accomplish. \n",
    "\n",
    "There is one more subtle issue. The token embedding vectors can be considered \n",
    "as a set of features that describe the token the embeddings represent. In the \n",
    "self-attention mechanism described above each of these features has equal \n",
    "weight in contribution to the attention scores (recall $a_n^* = x_n^Tx_n$ ). However, \n",
    "it may be benefically to allow some features to contribute more heavily to the weight scores.\n",
    "\n",
    "We can address both these issues by adding trainable parameters to our self-attention\n",
    "mechanism. Let $U$ be a $D \\times D$ matrix of trainable weights and define\n",
    "$$\n",
    "\\widetilde{\\mathbf{X}} = \\mathbf{XU}.\n",
    "$$\n",
    "\n",
    "Recall from linear algebra that every [linear transformation can be represented\n",
    "by a matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication). So this\n",
    "is simply applying a linear transformation to the matrix $\\mathbf{X}$. This is also \n",
    "the same thing as adding a \"layer\" to a artificial neural network.\n",
    "This will allow the model to \"learn\" how to weight the features in each of the\n",
    "token embedding vectors when computing the attention scores that will be used\n",
    "to compute the context vectors.\n",
    "\n",
    "The new unconstrianed attention scores using the new linear transformed token embedding vectors\n",
    "is given by\n",
    "$$\n",
    "\\widetilde{\\mathbf{A}}^* = \\widetilde{\\mathbf{X}} \\widetilde{\\mathbf{X}}^T = \\mathbf{XU}\\mathbf{U}^T\\mathbf{X}^T.\n",
    "$$\n",
    "\n",
    "Plugging these new unconstrianed attention scores into the formula for calculating the context vcetors gives\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}[\\widetilde{\\mathbf{A}}^*]\\widetilde{\\mathbf{X}} = \\text{Softmax}[\\widetilde{\\mathbf{X}} \\widetilde{\\mathbf{X}}^T]\\widetilde{\\mathbf{X}} = \\text{Softmax}[\\mathbf{XU}\\mathbf{U}^T\\mathbf{X}^T]\\mathbf{XU}.\n",
    "$$\n",
    "\n",
    "There are more improvements we can make to our attention scores. As summarized in \n",
    "Bishop & Bishop, 2023, the new unconstrianed attention scores, \n",
    "$\\widetilde{\\mathbf{A}}^* = \\mathbf{XU}\\mathbf{U}^T\\mathbf{X}^T$, are symmetric.\n",
    "However, there are many cases when we will want to scores to be asymmetric. For example,\n",
    "we **may** want MacBook to have a stronger association with Apple because the Apply company\n",
    "makes the MacBook than the associate from Apple to MacBook because Apple can be\n",
    "used in many more contexts. Another limitation is that the parameter matrix\n",
    "(the learned linear transformation) is used to both compute the attention scores\n",
    "and to transform the value vectors. We could get more flexibility by defining\n",
    "seperated transformed matrices, $\\mathbf{Q}$ for queries, $\\mathbf{K}$ for keys, \n",
    "and $\\mathbf{V}$ for values each with their own independent set of parameters:\n",
    "$$\n",
    "\\mathbf{Q} = \\mathbf{XW}_q, \\\\\n",
    "\\mathbf{K} = \\mathbf{XW}_k, \\\\\n",
    "\\mathbf{V} = \\mathbf{XW}_v.\n",
    "$$\n",
    "Using our independently linearly transformed query and key matrices, we get the following unconstrained attention scores\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{Q}\\mathbf{K}^T.\n",
    "$$\n",
    "Finally, using $\\mathbf{V}$ for the independently linearly transformed values and plugging in our unconstrained attention scores to the \n",
    "formula for the context vectors gives\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}[\\mathbf{Q}\\mathbf{K}^T]{\\mathbf{V}}.\n",
    "$$\n",
    "\n",
    "There is one last adjustment to make. The last thing we will do is scale\n",
    "the attention scores before applying the softmax. The softmax functions struggles\n",
    "to handle very large values so scaling the the attentions scores to make\n",
    "sure no values are too large before applying the softmax function to constrain the\n",
    "attention scores improves model training. We will normalize by the square root\n",
    "of the dimension of the value and key vectors, $D_k$. The reasoning behind this choice is\n",
    "that if all the elements of the query and key vectors were all independent random numbers\n",
    "with mean $0$ and variance $1$ then their dot product would have variance $D_k$. So\n",
    "we normalize by the standard deviation to again achieve unit variance. This final step results in the\n",
    "**scaled dot-produce self-attention** form of self-attention\n",
    "that is used in most modern language models:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}\\left[\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{D_k}}\\right]{\\mathbf{V}}.\n",
    "$$\n",
    "\n",
    "**The amazing and maybe the most important feature of the self-attention mechanism is that\n",
    "the attention scores, which act like a set of weights in the network (i.e. a learned \n",
    "linear transformation of $\\mathbf{V}$), depend on the input data.** \n",
    "This example may not seem like much but this is a rare feautre of\n",
    "artifical neural networks. In most artifical neural networks the weights in the \n",
    "network are fixed after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13008d65",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89083218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42711b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4418d5b",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Bishop, C. M., & Bishop, H. (2023). Deep Learning. Springer.  \n",
    "2. Raschka, S. (2024). Build a Large Language Model (From Scratch). Manning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
