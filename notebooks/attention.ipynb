{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381baf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13753c430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(239)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aadfead",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This educational notebook delves into the mathematics of self-attention and multi-head attention mechanisms that\n",
    "make up the Transformer architecture. It also provides implementations in PyTorch. \n",
    "This architecture is the core technology behind large language models (LLMs), \n",
    "such as OpenAI's GPT models.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Attention Weights](#Attention-Weights)\n",
    "2. [Self-Attention Mechanism Without Trainable Parameters](#Self-Attention-Mechanism-Without-Trainable-Parameters)\n",
    "3. [Adding Trainable Weights to Self-Attention](#Adding-Trainable-Weights-to-Self-Attention)\n",
    "4. [Positional Encoding](#Positional-Encoding)\n",
    "5. [Causal Attention](#Causal-aAttention)\n",
    "6. [Multi-head Attention](#Multi-head-Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f6a20",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d57ae4",
   "metadata": {},
   "source": [
    "  \n",
    "The input data to a transfomer is a set of vectors $x_1, \\cdots, x_N$ each with a\n",
    "dimensionality of $D$. This, of course, can we written as a $D \\times N$ matrix $\\mathbf{X}$. \n",
    "In (Bishop & Bishop, 2023) the $x_1 \\cdots x_n$ vectors are referred to as \"tokens.\" \n",
    "I will refer to these as token embedding vectors or token embeddings for short. \n",
    "In language modeling, through a process known as \"tokenization\" a sentence or \n",
    "sequence of words in split into a sequence of natural number which I will \n",
    "refer to as \"tokens.\" Each token is then converted to a $D$ dimenionsal \n",
    "vector (i.e. token embedding) to create the input for the transformer.\n",
    "\n",
    "Now suppose we have a sentence (e.g. \"I swam across the river to get to the other bank\") and this\n",
    "sentence has gone through the tokenization process and has been converted into a sequence of \n",
    "token embedding vectors, $x_1, \\cdots, x_N$, each with dimensionality of $D$. Now goal is to map this\n",
    "sequence of vectors $x_1, \\cdots, x_N$ to a new sequence of vectors $y_1, \\cdots, y_N$\n",
    "in a new space that captures important \"semantic\" information within the full sentence. \n",
    "For example, when reading the sentence \"I swam across the river to get to the other bank\" \n",
    "the words \"swam\" and \"river\" give us information about the meaning of \"bank\" in that sentence.\n",
    "As in (Raschka, 2024) we will refer to the $y_n$ as a context vector to highlight\n",
    "that each $y_n$ depends on the other token embeddings in the token embedding sequence or \"context.\"\n",
    "\n",
    "To this end, we want the context vector $y_n$ to depend not only on $x_n$ but also on\n",
    "the full token embedding sequence $x_1, \\cdots, x_N$. For those that more familar with\n",
    "tokenizers will ignore masking for now. Returning to our example sentence, \n",
    "this would mean the context vector $y_n$ would depend not only on the \n",
    "embedding vector $x_n$ corresponding to the word \"bank\" but on \n",
    "every embedding vector in the sequence, each corresponding to a word (i.e. token) \n",
    "in the sentence.\n",
    "\n",
    "A possible approach to constructing the context vectors $y_n$ is to define each $y_n$ \n",
    "to be a linear combination of the token embeddings $x_1, \\cdots n_N$. That is, $y_n = \\sum_{m=1}^N a_{nm}x_m$.\n",
    "The question then become how do we define the weights $a_{nm}$. To start we constrain the\n",
    "weights to be non-negative and sum to one. Mathematically, $a_{nm} \\geq 0$ for $n, m=1, \\cdots N$ and $\\sum_{m=1}^N a_{nm}=1$ for $n=1, \\cdots N$.\n",
    "\n",
    "In summary, we have\n",
    "$$\n",
    "y_n = \\sum_{m=1}^N a_{nm}x_m\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "a_{nm} \\geq 0 \\quad n, m=1, \\cdots N \\quad \\text{and}\\\\\n",
    "\\sum_{m=1}^N a_{nm}=1 \\quad n=1, \\cdots N.\n",
    "$$\n",
    "The $a_{nm}$ parameters are called **attention weights**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fab40",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism Without Trainable Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df68703",
   "metadata": {},
   "source": [
    "Self-Attention is the name given to a process for calculating the attention weights. \n",
    "Many of the terms used here are from the field of information retrieval and so we\n",
    "will began with some definitions. If you are familiar with Python then recall \n",
    "a dictionary object. Dictionaries in Python have **keys** and **values** (e.g. {'key': 'value'}).\n",
    "Now given a dictionary a user provides a query. The query is what they want to find in the dictionary. \n",
    "The query is used to look up (or check) for the key in the dictionary and when a match is made\n",
    "the value is returned. This looks like this in Python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5ecf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: self, Value: attention\n"
     ]
    }
   ],
   "source": [
    "example_dictionary = collections.defaultdict(str)\n",
    "example_dictionary['self'] = 'attention'  # {key1: value1}\n",
    "example_dictionary['attention'] = 'weights'  # {key2: value2}\n",
    "example_dictionary['python'] = 'dictionary'  # {key3: value3}\n",
    "example_dictionary['trainable'] = 'weights'  # {key3: value3}\n",
    "\n",
    "query = 'self'\n",
    "value = example_dictionary.get(query, 'not found')\n",
    "print(f'Query: {query}, Value: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1b55e",
   "metadata": {},
   "source": [
    "The Python dictionary example is just one simple example of the idea behind key, value, and query triples.\n",
    "You can imagine extending this so that the query and key do not have to match exactly as they did in our dictionary example.\n",
    "We could take our query and search for the most \"similar\" (we have to define what similar means). This is idea will help us understand\n",
    "the self-attention mechanism.\n",
    "\n",
    "In the self-attention mechanism the sequences of token embedding vectors, ${x_1, \\cdots x_N}$, can be thought of as values and these values will be used to create the context vectors. Furthermore, we will also use sequences of token embedding vectors,  ${x_1, \\cdots x_N}$, directly as the keys for each corresponding value. So using the dictionary example before you could imagine our dictionary looks like $\\{x_1:x_1, x_2:x_2, \\cdots, x_N:x_N\\}$. This example is to simply an analogy to help our understanding we will of course not acutally be using these dictionaries in the attention mechanism. Now consider a single token embedding vector, $x_m$, from our sequence of token embedding vectors, ${x_1, \\cdots x_N}$. The token embedding vector $x_m$ will be our query. For each $x_m$ will then measure the \"degree of match\" (or similarity) between the $x_m$ and all the keys (our sequence of token embedding vectors ${x_1, \\cdots x_N}$). This measure of \"degree of match\" will then be used as the weights in a linear combination of the values to produce the context embedding vector $y_m$. We will repeat this produce treating each embedding vector as a query and using it to find the weights to use in a linear combination of the values to produce the corresponding context embedding vector. Of course using matrix multiple we can calculate all the necessary weights at once.\n",
    "\n",
    "A common method to determine the degree of match (or similarity) between a query vector and a key vector is to take their [dot product](https://en.wikipedia.org/wiki/Dot_product). Let $a^*_{nm}$ be the unconstrained attention weights. We will refer to these as attention scores as in (Raschka, 2024). Then for a given query $x_n$ and key $x_m$ the attention score is defined to be $a^*_{nm} = x^T_n x_m$. To constrain the values to be non-negative and sum to one we will apply the [softmax function](https://en.wikipedia.org/wiki/Softmax_function). This results in the attention weights:\n",
    "$$\n",
    "a_{nm} = \\frac{\\exp(a^*_{nm})}{\\sum_{k=1}^N \\exp(a^*_{nk})} = \\frac{\\exp(x^T_n x_m)}{\\sum_{k=1}^N \\exp(x^T_n x_k)}.\n",
    "$$\n",
    "\n",
    "Each attention weight $a_{nm}$ is the contribution of the key $x_m$ to the output context vector $y_n$. Mathematically, \n",
    "$$\n",
    "y_n = \\sum_{m=1}^N a_{nm}x_m.\n",
    "$$\n",
    "\n",
    "Let $\\mathbf{X}$ be a matrix of token embedding vectors where each **row** corresponds to a token embedding, $x_n$ and let $\\mathbf{A^*}$ be the matrix of unconstrained attention scores (before the softmax function has been applied). Then we have $\\mathbf{A^*} = \\mathbf{X}\\mathbf{X^T}$ where each row of $A^*$ is a vector of scores that when constrained using the softmax function will determine how much each input token embedding vector contributes to the linear combination that produces the context vector $y_n$. Applying the softmax function across the rows we get $A = \\text{Softmax}[\\mathbf{XX^T}] = \\text{Softmax}[\\mathbf{A^*}]$ where the softmax function is apply across the rows independently. Finally we can apply this attention weights to the values (another set of the input token embeddings token embedding) using matrix multiplcation. Let $\\mathbf{Y}$ be the matrix of output context vectors where each **row** is a context vector in the new (hopefully) more contextually rich space. Then \n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}[\\mathbf{XX^t}]\\mathbf{X}  = \\text{Softmax}[\\mathbf{A^*}]\\mathbf{X} = \\mathbf{AX}.\n",
    "$$\n",
    "\n",
    "Now lets see how to program this in [PyTorch](https://pytorch.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b96f65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tensor([[0.5341, 0.3316, 0.5995],\n",
      "        [0.9891, 0.8921, 0.4602],\n",
      "        [0.1234, 0.5678, 0.9101],\n",
      "        [0.4567, 0.7890, 0.1234],\n",
      "        [0.2345, 0.6789, 0.3456]])\n",
      "\n",
      "A_star = tensor([[0.7546, 1.1000, 0.7998, 0.5795, 0.5576],\n",
      "        [1.1000, 1.9859, 1.0474, 1.2124, 0.9966],\n",
      "        [0.7998, 1.0474, 1.1659, 0.6167, 0.7289],\n",
      "        [0.5795, 1.2124, 0.6167, 0.8463, 0.6854],\n",
      "        [0.5576, 0.9966, 0.7289, 0.6854, 0.6353]])\n",
      "\n",
      "A = tensor([[0.1953, 0.2759, 0.2044, 0.1640, 0.1604],\n",
      "        [0.1564, 0.3793, 0.1484, 0.1750, 0.1410],\n",
      "        [0.1822, 0.2334, 0.2628, 0.1517, 0.1698],\n",
      "        [0.1578, 0.2971, 0.1637, 0.2060, 0.1754],\n",
      "        [0.1679, 0.2605, 0.1993, 0.1908, 0.1815]])\n",
      "\n",
      "Y = tensor([[0.5150, 0.6652, 0.5058],\n",
      "        [0.5899, 0.7082, 0.4736],\n",
      "        [0.4698, 0.6529, 0.5333],\n",
      "        [0.5335, 0.6919, 0.4664],\n",
      "        [0.5016, 0.6750, 0.4882]])\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have a sentence of 5 words or \"tokens\"\n",
    "# (e.g. \"This is an example sentence.\")\n",
    "# Then we can represent each token as a vector of dimension d (e.g. d=3).\n",
    "\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [0.5341, 0.3316, 0.5995],  # This        x1\n",
    "        [0.9891, 0.8921, 0.4602],  # is          x2\n",
    "        [0.1234, 0.5678, 0.9101],  # an          x3\n",
    "        [0.4567, 0.7890, 0.1234],  # example     x4\n",
    "        [0.2345, 0.6789, 0.3456],  # sentence    x5\n",
    "    ]\n",
    ")\n",
    "print(f'X = {X}\\n')\n",
    "\n",
    "A_star = X @ X.T  # Unconstrained attention scores\n",
    "print(f'A_star = {A_star}\\n')\n",
    "\n",
    "# Output:\n",
    "# A_star = tensor([[0.7546, 1.1000, 0.7998, 0.5795, 0.5576],\n",
    "#                  [1.1000, 1.9859, 1.0474, 1.2124, 0.9966],\n",
    "#                  [0.7998, 1.0474, 1.1659, 0.6167, 0.7289],\n",
    "#                  [0.5795, 1.2124, 0.6167, 0.8463, 0.6854],\n",
    "#                  [0.5576, 0.9966, 0.7289, 0.6854, 0.6353]])\n",
    "\n",
    "# Apply softmax across rows to constrain attention scores\n",
    "A = torch.softmax(A_star, dim=1)\n",
    "print(f'A = {A}\\n')\n",
    "\n",
    "# Output:\n",
    "# A = tensor([[0.1953, 0.2759, 0.2044, 0.1640, 0.1604],\n",
    "#         [0.1564, 0.3793, 0.1484, 0.1750, 0.1410],\n",
    "#         [0.1822, 0.2334, 0.2628, 0.1517, 0.1698],\n",
    "#         [0.1578, 0.2971, 0.1637, 0.2060, 0.1754],\n",
    "#         [0.1679, 0.2605, 0.1993, 0.1908, 0.1815]])\n",
    "#\n",
    "# Looking at the first row of A, we see that to compute y1, the context vector\n",
    "# for the first token embedding x1 (\"This\"), we will weight x1 (itself) by\n",
    "# 0.1953, x2 (\"is\") by 0.2759, x3 (\"an\") by 0.2044, x4 (\"example\") by 0.1640,\n",
    "# and x5 (\"sentence\") by 0.1604. This means that the context.\n",
    "\n",
    "# Compute the context vectors using matrix multiplication\n",
    "Y = A @ X\n",
    "print(f'Y = {Y}')  # notice that Y is the same shape as X\n",
    "\n",
    "# Y = tensor([[0.5150, 0.6652, 0.5058], # This      y1\n",
    "#         [0.5899, 0.7082, 0.4736],     # is        y2\n",
    "#         [0.4698, 0.6529, 0.5333],     # an        y3\n",
    "#         [0.5335, 0.6919, 0.4664],     # example   y4\n",
    "#         [0.5016, 0.6750, 0.4882]])    # sentence  y5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8ffe1",
   "metadata": {},
   "source": [
    "## Adding Trainable Parameters to Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6418a446",
   "metadata": {},
   "source": [
    "At this point the transformation from an token embedding vector, $x_1$ to a context\n",
    "vector $y_1$ is fixed with no ability to \"good\" representations from the data.\n",
    "To this end we will add weights that through training will allow us to learn\n",
    "good context context vectors, $y_1$. In this context \"good\" means the context vectors are useful\n",
    "for the task we want to accomplish. \n",
    "\n",
    "There is one more subtle issue. The token embedding vectors can be considered \n",
    "as a set of features that describe the token the embeddings represent. In the \n",
    "self-attention mechanism described above each of these features has equal \n",
    "weight in contribution to the attention scores (recall $a_n^* = x_n^Tx_n$ ). However, \n",
    "it may be benefically to allow some features to contribute more heavily to the weight scores.\n",
    "\n",
    "We can address both these issues by adding trainable parameters to our self-attention\n",
    "mechanism. Let $U$ be a $D \\times D$ matrix of trainable weights and define\n",
    "$$\n",
    "\\widetilde{\\mathbf{X}} = \\mathbf{XU}.\n",
    "$$\n",
    "\n",
    "Recall from linear algebra that every [linear transformation can be represented\n",
    "by a matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication). So this\n",
    "is simply applying a linear transformation to the matrix $\\mathbf{X}$. This is also \n",
    "the same thing as adding a \"layer\" to a artificial neural network.\n",
    "This will allow the model to \"learn\" how to weight the features in each of the\n",
    "token embedding vectors when computing the attention scores that will be used\n",
    "to compute the context vectors.\n",
    "\n",
    "The new unconstrianed attention scores using the new linear transformed token embedding vectors\n",
    "is given by\n",
    "$$\n",
    "\\widetilde{\\mathbf{A}}^* = \\widetilde{\\mathbf{X}} \\widetilde{\\mathbf{X}}^T = \\mathbf{XU}\\mathbf{U}^T\\mathbf{X}^T.\n",
    "$$\n",
    "\n",
    "Plugging these new unconstrianed attention scores into the formula for calculating the context vcetors gives\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}[\\widetilde{\\mathbf{A}}^*]\\widetilde{\\mathbf{X}} = \\text{Softmax}[\\widetilde{\\mathbf{X}} \\widetilde{\\mathbf{X}}^T]\\widetilde{\\mathbf{X}} = \\text{Softmax}[\\mathbf{XU}\\mathbf{U}^T\\mathbf{X}^T]\\mathbf{XU}.\n",
    "$$\n",
    "\n",
    "There are more improvements we can make to our attention scores. As summarized in \n",
    "Bishop & Bishop, 2023, the new unconstrianed attention scores, \n",
    "$\\widetilde{\\mathbf{A}}^* = \\mathbf{XU}\\mathbf{U}^T\\mathbf{X}^T$, are symmetric.\n",
    "However, there are many cases when we will want to scores to be asymmetric. For example,\n",
    "we **may** want MacBook to have a stronger association with Apple because the Apply company\n",
    "makes the MacBook than the associate from Apple to MacBook because Apple can be\n",
    "used in many more contexts. Another limitation is that the parameter matrix\n",
    "(the learned linear transformation) is used to both compute the attention scores\n",
    "and to transform the value vectors. We could get more flexibility by defining\n",
    "seperated transformed matrices, $\\mathbf{Q}$ for queries, $\\mathbf{K}$ for keys, \n",
    "and $\\mathbf{V}$ for values each with their own independent set of parameters:\n",
    "$$\n",
    "\\mathbf{Q} = \\mathbf{XW}^{(q)}, \\\\\n",
    "\\mathbf{K} = \\mathbf{XW}^{(k)}, \\\\\n",
    "\\mathbf{V} = \\mathbf{XW}^{(v)}.\n",
    "$$\n",
    "Using our independently linearly transformed query and key matrices, we get the following unconstrained attention scores\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{Q}\\mathbf{K}^T.\n",
    "$$\n",
    "Finally, using $\\mathbf{V}$ for the independently linearly transformed values and plugging in our unconstrained attention scores to the \n",
    "formula for the context vectors gives\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}[\\mathbf{Q}\\mathbf{K}^T]{\\mathbf{V}}.\n",
    "$$\n",
    "\n",
    "There is one last adjustment to make. The last thing we will do is scale\n",
    "the attention scores before applying the softmax. The softmax functions struggles\n",
    "to handle very large values so scaling the the attentions scores to make\n",
    "sure no values are too large before applying the softmax function to constrain the\n",
    "attention scores improves model training. We will normalize by the square root\n",
    "of the dimension of the value and key vectors, $D_k$. The reasoning behind this choice is\n",
    "that if all the elements of the query and key vectors were all independent random numbers\n",
    "with mean $0$ and variance $1$ then their dot product would have variance $D_k$. So\n",
    "we normalize by the standard deviation to again achieve unit variance. This final step results in the\n",
    "**scaled dot-produce self-attention** form of self-attention\n",
    "that is used in most modern language models:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}\\left[\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{D_k}}\\right]{\\mathbf{V}}.\n",
    "$$\n",
    "\n",
    "**The amazing and maybe the most important feature of the self-attention mechanism is that\n",
    "the attention scores, which act like a set of weights in the network (i.e. a learned \n",
    "linear transformation of $\\mathbf{V}$), depend on the input data.** \n",
    "This example may not seem like much but this is a rare feautre of\n",
    "artifical neural networks. In most artifical neural networks the weights in the \n",
    "network are fixed after training.\n",
    "\n",
    "Now lets implement this in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef65f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      "tensor([[0.1981, 0.1991, 0.2002, 0.2013, 0.2013],\n",
      "        [0.2040, 0.1963, 0.2096, 0.1923, 0.1977],\n",
      "        [0.2006, 0.2032, 0.2054, 0.1943, 0.1965],\n",
      "        [0.2070, 0.1984, 0.2124, 0.1877, 0.1944],\n",
      "        [0.2051, 0.2005, 0.2102, 0.1895, 0.1947]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Output Context Vectors of Self-Attention:\n",
      "tensor([[ 0.3362, -0.4088,  0.2517],\n",
      "        [ 0.3290, -0.4012,  0.2486],\n",
      "        [ 0.3344, -0.4069,  0.2510],\n",
      "        [ 0.3276, -0.3996,  0.2480],\n",
      "        [ 0.3299, -0.4021,  0.2490]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Adapted from Section 3.4 from Raschka, S. (2024).\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # Recall linear layers, nn.Linear, are linear transformations which\n",
    "        # are simply matrices.\n",
    "        self.D_k = d_out\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Apply linear transformations to the input X to compute the\n",
    "        # query, key, and value matrices\n",
    "        Q = self.W_q(X)  # Q = XW_q\n",
    "        K = self.W_k(X)  # K = XW_k\n",
    "        V = self.W_v(X)  # V = XW_v\n",
    "\n",
    "        # Compute attention weights\n",
    "        # A = Softmax[Q K^t / sqrt(D_k)]\n",
    "        attention_weights = torch.softmax(Q @ K.T / self.D_k**0.5, dim=-1)\n",
    "        print(f'Attention Weights:\\n{attention_weights}\\n')\n",
    "\n",
    "        # Apply attention weights to the value matrix V to\n",
    "        # compute the context vectors Y\n",
    "        Y = attention_weights @ V\n",
    "\n",
    "        return Y\n",
    "\n",
    "\n",
    "# Example usage\n",
    "d_in = 3  # Dimension of input token embedding vectors\n",
    "d_out = 3  # Dimension of output context vectors\n",
    "\n",
    "self_attention = SelfAttentionV1(d_in, d_out)\n",
    "output = self_attention(X)\n",
    "print(f'Output Context Vectors of Self-Attention:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596353bb",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee188194",
   "metadata": {},
   "source": [
    "Given the current formulation of the self-attention mechanism, the ordering of \n",
    "the token embedding vectors has no influence on the attention weights. One approach\n",
    "to incorporating positional information is to define a position vector for each\n",
    "position. These vectors could be concatenated with the token embedding vector however\n",
    "this would dramatically increase the number of parameters in the model and therefore\n",
    "increase the comutational cost. Another option is to force the postional vectors\n",
    "to be the same demension as the token embedding vectors and then add them together.\n",
    "Let $\\mathbf{r}_n$ we the position vector for the $n$th position and let $\\widetilde{\\mathbf{x}}_n$ be\n",
    "the token embedding vector, $\\mathbf{x}_n$ added with the position vector, \n",
    "$$\n",
    "\\widetilde{\\mathbf{x}}_n = \\mathbf{x}_n + \\mathbf{r}_n.\n",
    "$$\n",
    "There are many different variants of position vectors and they are typically categorized\n",
    "into two different groups. The first are **absolute** position vectors, where each \n",
    "position $n$ is assigned a unique, fixed vector $r_n$. A weakness of absolute position vectors\n",
    "is that they do not generalize to sequences longer than the ones used during training. \n",
    "The second group comprises **relative** position vectors. This group allows the model\n",
    "to generalize to sequences of previously  unobserved sequence lengths.\n",
    "Instead of encoding the absolute position, these vectors encode the relationship between two positions. \n",
    "For example, the attention mechanism might consider the relative distance between a query token at position i and a key token at position j.\n",
    "\n",
    "We will follow the same approach as used in OpenAI's series of GPT models while knowledging their weaknesses. The \n",
    "GPT models learn absolute position vectors jointly with tge rest of the model weights. Since these are \n",
    "absolute position vectors the will not generalize to sequences of unobserved length since the\n",
    "position vectors will be untrained for any position not observed during training.\n",
    "\n",
    "Now lets add these position vectors to our self-attention mechanism.\n",
    "â€‹\t\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2501dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      "tensor([[0.1458, 0.0471, 0.2460, 0.2505, 0.3105],\n",
      "        [0.0947, 0.0834, 0.3150, 0.1842, 0.3226],\n",
      "        [0.2007, 0.4527, 0.0939, 0.1501, 0.1026],\n",
      "        [0.1880, 0.0915, 0.2341, 0.2317, 0.2547],\n",
      "        [0.2253, 0.2973, 0.1442, 0.1847, 0.1486]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Output Context Vectors of Self-Attention:\n",
      "tensor([[ 0.0295,  0.0785,  0.1139],\n",
      "        [ 0.0946,  0.1900,  0.1150],\n",
      "        [-0.2950,  0.1823, -0.1397],\n",
      "        [-0.0281,  0.0799,  0.0833],\n",
      "        [-0.2052,  0.1193, -0.0484]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # Recall linear layers, nn.Linear, are linear transformations which\n",
    "        # are simply matrices.\n",
    "        self.D_k = d_out\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # POSITIONAL ENCODING\n",
    "        self.positional_encoding = nn.Embedding(context_len, d_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        seq_len, _ = X.shape  # Get the sequence length from the input X\n",
    "\n",
    "        # Apply positional encoding to the input X\n",
    "        X = X + self.positional_encoding(\n",
    "            torch.arange(seq_len, device=X.device)\n",
    "        )\n",
    "\n",
    "        # Apply linear transformations to the input X to compute the\n",
    "        # query, key, and value matrices\n",
    "        Q = self.W_q(X)  # Q = XW_q\n",
    "        K = self.W_k(X)  # K = XW_k\n",
    "        V = self.W_v(X)  # V = XW_v\n",
    "\n",
    "        # Compute attention weights\n",
    "        # A = Softmax[Q K^t / sqrt(D_k)]\n",
    "        attention_weights = torch.softmax(Q @ K.T / self.D_k**0.5, dim=-1)\n",
    "        print(f'Attention Weights:\\n{attention_weights}\\n')\n",
    "\n",
    "        # Apply attention weights to the value matrix V to\n",
    "        # compute the context vectors Y\n",
    "        Y = attention_weights @ V\n",
    "\n",
    "        return Y\n",
    "\n",
    "\n",
    "# Example usage\n",
    "d_in = 3  # Dimension of input token embedding vectors\n",
    "d_out = 3  # Dimension of output context vectors\n",
    "\n",
    "self_attention = SelfAttention(d_in, d_out, context_len=X.shape[0])\n",
    "output = self_attention(X)\n",
    "print(f'Output Context Vectors of Self-Attention:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be7310",
   "metadata": {},
   "source": [
    "## Causal Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8161d",
   "metadata": {},
   "source": [
    "Foundational large language models are pre-trained on a one-token-ahead prediction task. \n",
    "Consider the sentence ``This is an example sentence`` and suppose we have only observed \n",
    "a sub-sequence of this sentence (e.g. ``This is an``) and we then want to predict the\n",
    "next word, ``example``. In order to not cheat at this task we need to make sure we\n",
    "only consider the words up-to the current word in our sub-sequence, ``This is an``.\n",
    "This is the idea of causal attention or masked attention. In causal attention we\n",
    "will restrict the model to only consider the previous and current inputs when\n",
    "computing the attention weights. \n",
    "\n",
    "Luckily, acheiving this in practice is simple. We can simple set all of the attention\n",
    "weights above the diagonal (these correspond the \"future\" inputs in the sequence) in our attention weight matrix to zero\n",
    "and then renormalize. the weights to sum to one. When we code this in PyTorch\n",
    "we will actually set values above the diagonal to \"-inf\" before we apply this softmax. The softmax\n",
    "of \"-inf\" is of course zero. This is a simplier implementation in PytTorch.\n",
    "\n",
    "**What about information leakage?** There is a subtle reason why this process does not\n",
    "result in information leakage. You may notice the attention scores matrix was\n",
    "orginially computed using the all sequence of inputs then we set the values above\n",
    "the dialog to -inf so it might appear that there is information about future\n",
    "inputs in our attention scores. However, since we normalize the attention\n",
    "scores after setting values above the diagonal to -inf this ensures that information\n",
    "from future inputs is destroyed (or not used) in the attention scores.\n",
    "\n",
    "Let now add this masking process to our self-attention class in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45849acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores before masking:\n",
      "tensor([[-0.4652, -1.4155, -0.3993, -0.4332, -0.1389],\n",
      "        [ 0.1301,  0.3980,  0.1119,  0.1782,  0.0268],\n",
      "        [-0.1884, -0.5737, -0.1618, -0.1677, -0.0579],\n",
      "        [-0.3804, -0.7427, -0.2699, -0.3657, -0.1499],\n",
      "        [-0.2941, -0.9808, -0.2641, -0.2768, -0.0792]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Attention Scores after masking:\n",
      "tensor([[-0.4652,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1301,  0.3980,    -inf,    -inf,    -inf],\n",
      "        [-0.1884, -0.5737, -0.1618,    -inf,    -inf],\n",
      "        [-0.3804, -0.7427, -0.2699, -0.3657,    -inf],\n",
      "        [-0.2941, -0.9808, -0.2641, -0.2768, -0.0792]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Constrained masked attention weights:\n",
      "tensor([[-0.4652,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1301,  0.3980,    -inf,    -inf,    -inf],\n",
      "        [-0.1884, -0.5737, -0.1618,    -inf,    -inf],\n",
      "        [-0.3804, -0.7427, -0.2699, -0.3657,    -inf],\n",
      "        [-0.2941, -0.9808, -0.2641, -0.2768, -0.0792]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Output Context Vectors of Causal Attention:\n",
      "tensor([[ 0.2368, -0.4407, -0.5991],\n",
      "        [ 0.8297, -0.1576, -0.1487],\n",
      "        [ 0.5685, -0.1990, -0.2292],\n",
      "        [ 0.2726, -0.0305, -0.3969],\n",
      "        [ 0.2189, -0.1285, -0.3952]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Adapted from Section 3.5 from Raschka, S. (2024).\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # Recall linear layers, nn.Linear, are linear transformations which\n",
    "        # are simply matrices.\n",
    "        self.D_k = d_out\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.positional_encoding = nn.Embedding(context_len, d_out)\n",
    "\n",
    "        # Respister buffer for the causal mask to ensure the mask matrix\n",
    "        # is not considered a trainable parameter\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(\n",
    "                torch.ones((context_len, context_len), dtype=torch.bool),\n",
    "                diagonal=1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        seq_len, d_in = X.shape\n",
    "\n",
    "        # Apply positional encoding to the input X\n",
    "        X = X + self.positional_encoding(\n",
    "            torch.arange(seq_len, device=X.device)\n",
    "        )\n",
    "\n",
    "        # Apply linear transformations to the input X to compute the\n",
    "        # query, key, and value matrices\n",
    "        Q = self.W_q(X)  # Q = XW_q\n",
    "        K = self.W_k(X)  # K = XW_k\n",
    "        V = self.W_v(X)  # V = XW_v\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = Q @ K.T\n",
    "\n",
    "        print(f'Attention Scores before masking:\\n{attention_scores}\\n')\n",
    "\n",
    "        # Apply the causal mask to the attention scores\n",
    "        # This ensures that information from future inputs is not used\n",
    "        # in the attention scores.\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask[:seq_len, :seq_len],\n",
    "            -torch.inf,  # type: ignore\n",
    "        )\n",
    "\n",
    "        print(f'Attention Scores after masking:\\n{attention_scores}\\n')\n",
    "\n",
    "        # A = Softmax[Q K^t / sqrt(D_k)]\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / self.D_k**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        print(f'Constrained masked attention weights:\\n{attention_scores}\\n')\n",
    "\n",
    "        # Apply attention weights to the value matrix V to\n",
    "        # compute the context vectors Y\n",
    "        Y = attention_weights @ V\n",
    "\n",
    "        return Y\n",
    "\n",
    "\n",
    "# Example usage\n",
    "d_in = 3  # Dimension of input token embedding vectors\n",
    "d_out = 3  # Dimension of output context vectors\n",
    "\n",
    "causal_attention = CausalAttention(d_in, d_out, context_len=5)\n",
    "output = causal_attention(X)\n",
    "print(f'Output Context Vectors of Causal Attention:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13008d65",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864d296",
   "metadata": {},
   "source": [
    "A single self-attention mechanism in a artificial nueral network is called an \"attention head.\"\n",
    "In a single attention head, through the single set of learned query, key and value matrices the model\n",
    "can be limited in the set of combinations of features it can construct. If several \n",
    "different combinations of features might be important a single attention \n",
    "head will average over these combinations. \n",
    "\n",
    "To this end, we can use multiple attention heads in parallel. Where each attention\n",
    "head will have is own independent set of query, key and value matrices. This will \n",
    "allow use to construct different combinations of features in each attention head \n",
    "and therefore focus on different aspects of the input. Suppose we have $H$ attention\n",
    "heads indexed by $h=1, \\cdots, H$ each with the same form as dervied above:\n",
    "$$\n",
    "\\mathbf{H}_h =  \\text{Softmax}\\left[\\frac{\\mathbf{Q}_h\\mathbf{K}^T_h}{\\sqrt{D_k}}\\right]{\\mathbf{V}_h}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{Q}_h = \\mathbf{XW}^{(q)}_h, \\\\\n",
    "\\mathbf{K}_h = \\mathbf{XW}^{(k)}_h, \\\\\n",
    "\\mathbf{V}_h = \\mathbf{XW}^{(v)}_h. \\\\\n",
    "$$\n",
    "That is, each attention head gets their own triple of learnable weight matrices for queries, keys and values, $\\mathbf{W}^{(i)}_h$ for $i \\in \\{q, k, v\\}$ and $h=1, \\cdots, H$. The $H$ attention heads each with dimension $N \\times D_v$ are concatenated into a single matrix then linearly transformed with another matrix $\\mathbf{W}^{(o)}$ of learnable parameters. This gives the matrix of context vectors\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Concat}\\left[\\mathbf{H}_1 \\cdots  \\mathbf{H}_H\\right]\\mathbf{W}^{(o)}.\n",
    "$$\n",
    "SInce each attention head, $\\mathbf{H}_h$ has  dimension $N \\times D_v$ the concatenated matrix had dimension $N \\times HD_v$ and therefore the matrix  $\\mathbf{W}^{(o)}$ has dimension $HD_v \\times D$ to allow for matrix multiplication an produce the context vector matrix of desired size (i.e. $N \\times D$). In multi0head attention $D_v$ is typcially chosen to equal $D/H$ so that the resulting concatenated matrix has dimension $N \\times D$.\n",
    "\n",
    "Now lets code this up in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac5d60bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores before masking:\n",
      "tensor([[ 0.0691, -0.1044, -0.0681, -0.1564,  0.0174],\n",
      "        [ 0.5019,  0.0876,  1.3337, -0.4958,  0.2746],\n",
      "        [ 0.1776, -0.0414,  0.2506, -0.2281,  0.0801],\n",
      "        [ 0.3385,  0.1831,  1.1695, -0.2406,  0.2070],\n",
      "        [ 0.0517, -0.0381,  0.0312, -0.0867,  0.0198]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Attention Scores after masking:\n",
      "tensor([[ 0.0691,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.5019,  0.0876,    -inf,    -inf,    -inf],\n",
      "        [ 0.1776, -0.0414,  0.2506,    -inf,    -inf],\n",
      "        [ 0.3385,  0.1831,  1.1695, -0.2406,    -inf],\n",
      "        [ 0.0517, -0.0381,  0.0312, -0.0867,  0.0198]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Constrained masked attention weights:\n",
      "tensor([[ 0.0691,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.5019,  0.0876,    -inf,    -inf,    -inf],\n",
      "        [ 0.1776, -0.0414,  0.2506,    -inf,    -inf],\n",
      "        [ 0.3385,  0.1831,  1.1695, -0.2406,    -inf],\n",
      "        [ 0.0517, -0.0381,  0.0312, -0.0867,  0.0198]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Attention Scores before masking:\n",
      "tensor([[-0.4541, -0.1715,  1.1298, -0.3720,  1.1024],\n",
      "        [-0.5405,  0.3134,  2.2659, -0.7384,  2.4577],\n",
      "        [ 0.8027,  1.5986,  0.4528, -0.2137,  1.0895],\n",
      "        [-0.4618, -0.8112, -0.1159,  0.1052, -0.4440],\n",
      "        [ 0.6754,  1.8433,  1.3170, -0.5092,  2.0777]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Attention Scores after masking:\n",
      "tensor([[-0.4541,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.5405,  0.3134,    -inf,    -inf,    -inf],\n",
      "        [ 0.8027,  1.5986,  0.4528,    -inf,    -inf],\n",
      "        [-0.4618, -0.8112, -0.1159,  0.1052,    -inf],\n",
      "        [ 0.6754,  1.8433,  1.3170, -0.5092,  2.0777]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Constrained masked attention weights:\n",
      "tensor([[-0.4541,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.5405,  0.3134,    -inf,    -inf,    -inf],\n",
      "        [ 0.8027,  1.5986,  0.4528,    -inf,    -inf],\n",
      "        [-0.4618, -0.8112, -0.1159,  0.1052,    -inf],\n",
      "        [ 0.6754,  1.8433,  1.3170, -0.5092,  2.0777]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Output Context Vectors of Multihead Attention:\n",
      "tensor([[-0.1915, -0.3467, -0.0262],\n",
      "        [-0.4815, -0.4151,  0.0550],\n",
      "        [-0.4658, -0.3932,  0.0656],\n",
      "        [-0.2696, -0.1788,  0.0309],\n",
      "        [-0.5278, -0.3201,  0.1067]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Adapted from Section 3.6 from Raschka, S. (2024).\n",
    "# Note: For educational purposes, this implementation is not optimized.\n",
    "# Simply concatenating the outputs of multiple heads is not an\n",
    "# efficient way to implement multi-head attention. See section\n",
    "# 3.6.2 in Raschka, S. (2024) for a more efficient implementation where\n",
    "# the heads are processed in parallel.\n",
    "\n",
    "\n",
    "class MultiHeadAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(d_in, d_out, context_len, qkv_bias)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.W_o = nn.Linear(num_heads * d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Apply each head to the input X and concatenate the results\n",
    "        head_outputs = torch.cat([head(X) for head in self.heads], dim=-1)\n",
    "        Y = self.W_o(head_outputs)\n",
    "\n",
    "        return Y\n",
    "\n",
    "\n",
    "# Example usage\n",
    "d_in = 3  # Dimension of input token embedding vectors\n",
    "d_out = 3  # Dimension of output context vectors\n",
    "\n",
    "# You will see the printed statements from the CausalAttention class\n",
    "# twice because MultiHeadAttention uses CausalAttention for each head.\n",
    "mha = MultiHeadAttentionV1(d_in, d_out, context_len=5, num_heads=2)\n",
    "output = mha(X)\n",
    "print(f'Output Context Vectors of Multihead Attention:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d6ea4",
   "metadata": {},
   "source": [
    "We can make our implementation more efficient. A single query, key, and value matrix triple.\n",
    "Consider the case of the query weight matrix (but the logic is the same for all three matrices).\n",
    "You can conceptually think of the large, $\\mathbf{W}_Q$, matrix as a stack of \n",
    "the individual head weight matrices, $\\mathbf{W}_{Q1}, \\mathbf{W}_{Q2}, \\cdots, \\mathbf{W}_{QK}$ . That is,\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_Q = \\left[\\mathbf{W}_{Q1} \\ \\mathbf{W}_{Q2} \\ \\cdots \\ \\mathbf{W}_{QH} \\right], \\\\\n",
    "\\mathbf{W}_K = \\left[\\mathbf{W}_{K1} \\ \\mathbf{W}_{K2} \\ \\cdots \\ \\mathbf{W}_{KH} \\right], \\\\\n",
    "\\mathbf{W}_V = \\left[\\mathbf{W}_{V1} \\ \\mathbf{W}_{V2} \\ \\cdots \\ \\mathbf{W}_{VH} \\right]. \\\\\n",
    "$$\n",
    "\n",
    "Then its easy to see that\n",
    "\n",
    "$$\n",
    "\\mathbf{XW}_Q = \\left[\\mathbf{XW}_{Q1} \\ \\mathbf{XW}_{Q2} \\ \\cdots \\ \\mathbf{XW}_{QH} \\right], \\\\\n",
    "\\mathbf{XW}_K = \\left[\\mathbf{XW}_{K1} \\ \\mathbf{XW}_{K2} \\ \\cdots \\ \\mathbf{XW}_{KH} \\right], \\\\\n",
    "\\mathbf{XW}_V = \\left[\\mathbf{XW}_{V1} \\ \\mathbf{XW}_{V2} \\ \\cdots \\ \\mathbf{XW}_{VH} \\right]. \\\\\n",
    "$$\n",
    "\n",
    "Using the ``.view`` method we can essentially unstack these matrices. From there \n",
    "we can use PyTorch's [broadcasting](https://docs.pytorch.org/docs/stable/notes/broadcasting.html)\n",
    "semantics to calculate the attention weights and apply the causal mask.\n",
    "\n",
    "With this we can implement the Multi-head attention mechanism as it is commonly\n",
    "used in the Transformer architecture that is used in Large Language Model.\n",
    "Though these classes do not work for batched inputs but that fix is easy to\n",
    "implement and as your math teacher might say is left as an exercise to the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5bf0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, 'd_out must be divisible by num_heads'\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.W_o = nn.Linear(d_out, d_out)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(\n",
    "                torch.ones((context_len, context_len), dtype=torch.bool),\n",
    "                diagonal=1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        num_tokens, d_in = X.shape\n",
    "\n",
    "        # stack the query, key, and value matrices\n",
    "        keys = self.W_k(X)\n",
    "        queries = self.W_q(X)\n",
    "        values = self.W_v(X)\n",
    "\n",
    "        # Unstack the query, key, and value matrices\n",
    "        # into multiple heads.\n",
    "        # keys, queries, and values will now have shape\n",
    "        # (num_tokens, num_heads, head_dim). Recall that\n",
    "        # head_dim = D // num_heads\n",
    "        keys = keys.view(num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # swap the first two dimensions to have shape\n",
    "        # (num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(-3, -2)\n",
    "        queries = queries.transpose(-3, -2)\n",
    "        values = values.transpose(-3, -2)\n",
    "\n",
    "        # Use PyTorch's broadcasting semantics to compute attention scores\n",
    "        # for each head in parallel.\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        concat_heads = (attn_weights @ values).transpose(1, 2)\n",
    "        concat_heads = concat_heads.contiguous().view(num_tokens, self.d_out)\n",
    "        Y = self.W_o(concat_heads)\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8462bdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Context Vectors of Multihead Attention:\n",
      "tensor([[-0.2345,  0.1695, -0.2975, -0.0606,  0.4637, -0.3039],\n",
      "        [-0.1418,  0.1676, -0.2328, -0.1002,  0.3881, -0.2657],\n",
      "        [-0.2019,  0.3206, -0.1069, -0.3274,  0.4580, -0.1621],\n",
      "        [-0.0782,  0.2902, -0.2329, -0.1236,  0.1868, -0.3234],\n",
      "        [-0.1032,  0.4003, -0.1043, -0.4110,  0.2032, -0.0542]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "d_in = 6  # Dimension of input token embedding vectors\n",
    "d_out = 6  # Dimension of output context vectors\n",
    "context_len = 5  # Length of the context (number of tokens)\n",
    "\n",
    "inputs = torch.rand(context_len, d_in)\n",
    "\n",
    "# You will see the printed statements from the CausalAttention class\n",
    "# twice because MultiHeadAttention uses CausalAttention for each head.\n",
    "mha = MultiHeadAttention(d_in, d_out, context_len, num_heads=2)\n",
    "output = mha(inputs)\n",
    "print(f'Output Context Vectors of Multihead Attention:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4418d5b",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Bishop, C. M., & Bishop, H. (2023). Deep Learning. Springer.  \n",
    "2. Raschka, S. (2024). Build a Large Language Model (From Scratch). Manning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
