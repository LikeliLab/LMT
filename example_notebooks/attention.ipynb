{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381baf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from lm.attention import CausalAttention, MultiHeadAttention, SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6989328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0605, -0.0033],\n",
      "         [ 0.0730, -0.1587],\n",
      "         [ 0.0059, -0.0838],\n",
      "         [ 0.0672, -0.0157]],\n",
      "\n",
      "        [[ 0.1103,  0.1985],\n",
      "         [ 0.0171,  0.1990],\n",
      "         [-0.0508,  0.1564],\n",
      "         [-0.0349,  0.2016]]], grad_fn=<UnsafeViewBackward0>)\n",
      "SelfAttention output shape: torch.Size([2, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_in = 6  # input dimension\n",
    "inputs = torch.randn(batch_size, seq_len, d_in)  # (batch_size, seq_len, d_in)\n",
    "\n",
    "sa = SelfAttention(d_in=d_in, d_out=2)\n",
    "print(sa(inputs))\n",
    "print('SelfAttention output shape:', sa(inputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "929881b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3349,  0.2773],\n",
      "         [-0.0405,  0.3809],\n",
      "         [-1.1918, -0.0232],\n",
      "         [ 0.1500, -0.0470]],\n",
      "\n",
      "        [[-0.0974,  1.1801],\n",
      "         [-0.5743,  1.8542],\n",
      "         [-0.3454,  1.1430],\n",
      "         [-0.0260,  0.2978]]], grad_fn=<UnsafeViewBackward0>)\n",
      "CausalAttention output shape: torch.Size([2, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_in = 6  # input dimension\n",
    "inputs = torch.randn(batch_size, seq_len, d_in)  # (batch_size, seq_len, d_in)\n",
    "\n",
    "ca = CausalAttention(d_in=d_in, d_out=2, context_length=4, dropout=0.1)\n",
    "print(ca(inputs))\n",
    "print('CausalAttention output shape:', ca(inputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f17a410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2387, -0.4613],\n",
      "         [-0.0111, -0.2861],\n",
      "         [ 0.0988, -0.2150],\n",
      "         [ 0.2050, -0.2337]],\n",
      "\n",
      "        [[ 0.1531, -0.4876],\n",
      "         [ 0.0352, -0.5687],\n",
      "         [-0.0486, -0.5851],\n",
      "         [-0.0200, -0.5280]]], grad_fn=<ViewBackward0>)\n",
      "MultiHeadAttention output shape: torch.Size([2, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_in = 6  # input dimension\n",
    "inputs = torch.randn(batch_size, seq_len, d_in)  # (batch_size, seq_len, d_in)\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=d_in, d_out=2, context_length=4, dropout=0.0, num_heads=2\n",
    ")\n",
    "print(mha(inputs))\n",
    "print('MultiHeadAttention output shape:', mha(inputs).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
